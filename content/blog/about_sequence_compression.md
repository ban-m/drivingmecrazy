+++
title = "塩基配列の圧縮について"
date = 2019-05-20
draft = false
tags = []
# category = "diary"
order = 0
weight = 0
aliases = []
template = "page.html"
[extra]
+++


## これは何？
DNAシークエンサーの出力を圧縮することについてのメモ。
対象は

- ゲノムを扱っている
- バイオインフォマティシャンで
- 配列の圧縮に興味がある

人。いくつかの基本的な考え方と、現状の圧縮アルゴリズムの方針について簡単に述べる。
配列圧縮の有用性については**書かない**。（そもそも、配列なんて圧縮しなくても、二次記憶は十分にある、という考え方は完全に妥当だし、私もこの意見には賛同する。一方で、配列の圧縮は、HDDやSSDにどうやって保存するのか、という問題以上の価値が実はある）

基本的には、DNAシークエンサーとしてはilluminaのWGSデータを考える。つまり、

- リードはだいたい100塩基で、
- エラー率は1％くらいで、
- あるゲノムがあって、そこから（バイアスはあるにせよ）サンプリングされている

という状況だ。もう少し、プロ出ない人向けに言い直すと、

- DNAシークエンサーは、DNA全長を読めるわけではなく、断片をちょっと読むのと大量に繰り返すのだが、
- その一回の読み取り量が100塩基くらいで、
- 読み取りを失敗する確率が、塩基あたり1%くらいで、
- 事前に「これは人のゲノムを読んでいます」「トマトです」と言った事前知識がある

ということだ。

<!-- more -->

## 基本的な考え方

まず、**完璧な圧縮**というものは存在しない。正確には、『どんな圧縮アルゴリズムとどんな入力に対しても、そのアルゴリズム以上の圧縮率を達成できる』ようなアルゴリズムは存在しない。こまかいことは省くが、これが達成できてしまうと、プログラムを最短の文字数で表すことが可能になってしまい、コルモゴロフ複雑性が計算できてしまうためだ。

よって、我々が考えるのは、**よく見る配列に対して、どのくらい圧縮できるか**だ。このような考え方の元では、一般的な圧縮アルゴリズム/ソフトウェア、つまり、BWTやbzip2といったものではなく、DNAシークエンサーに対して特注のアルゴリズム/ソフトウェアを設計するのは、意味がある研究領域になる。

また、**このような状況下では、アルゴリズムの評価が出来ない**というのは正しくない。実際は、ナイーブな表現というものがあって（要するに、『全部utf-8/ASCIIで書いたときのファイルサイズ』）、そこからどのくらいよくなるかは、入力のフォーマットや性質を仮定すれば計算できるし、これを用いれば、二種類のアルゴリズムの比較が出来ることもある。

## よく使われるアルゴリズム

一般的な圧縮と同じように、配列の圧縮も、大きく分けると、二種類に分かれる。つまり、

1. 統計的な方法
2. 置換による方法

だ。一つ目の統計的な方法とは、例えば、『頻度が高い要素には短いコードを、頻度が低いコードには長いコードを』割り当てるハフマン符号化のように、入力の統計的性質を用いる方法だ。
個人的には、入力に対して、前処理を行って下流に流すものも、多くはここに含まれると感じる。

### 統計的な方法

最も単純だが、そこそこの効果を上げる例として、<i>bioinformatics</i>において出版された"[LFQC: a lossless compression algorithm for FASTQ files](https://doi.org/10.1093/bioinformatics/btv384)"をあげよう。これは、Illuminaの出力するファイル形式(.fastq)が、規則的なコーディングを採用していることを利用している。

fastqファイルは改行区切りのレコードであり、各レコードは、

1. レコードID
2. ATGC上の文字列
3. 区切り文字
4. クオリティスコア（2．と同じ長さで、2．の各塩基の信頼度を表す）

となっている。NBFでかくと：

```
<fastq> ::= {<record> "\n"}* <record>
<record> ::= <Header> "\n" <Sequence> "\n" "+" <Queality>
<Header> ::= String
<Sequence> ::= {'A' | 'C' | 'G' | 'T'}*
<Quality> ::= {x | x is a ascii character with the code in 33..73}*
```

となる。



当然、上記のNBFは**一般的すぎる**ことが多い。つまり、**大体、ヘッダーというのはどれも似たような見た目をしており、クオリティスコアというのはだいたい20くらいを取るものだ**という事前知識を我々は持っている。これを用いることで、少し圧縮できる。

例えば、ヘッダーは 機器名\_シークエンシング日時\_メタデータみたいな見た目をしている。また、クオリティスコアを明示的に持つのではなく、一つ前のスコアとの差分を持つと、最大でも\\( \log \lceil \max (q_i - q_{i+1}) \rceil \\) bitあればよいことになる。もちろん、明示的にやらないで、算術符号(arithmetic encoding)を用いてもいい。

ここまで読んで、「そんなん塩基の圧縮じゃないじゃん、詐欺だ」と思うのは完全に正しい。実際、これは塩基ではなく、ヘッダーやメタデータを圧縮することになっている。一方で、どういうわけか、ほとんどのfastqファイルのヘッダーやメタデータは、配列そのものと同じくらい大きいので、実際は、全データの2/3近くを扱えていることになる。

### 置換による方法

私がより重要だと思うのが、この置換による圧縮だ。取り上げる論文としては、同様に<i>bioinformatics</i>で出版されている"
[Disk-based compression of data from genome sequencing](https://doi.org/10.1093/bioinformatics/btu844)"を取り上げよう。

この論文のコアとなる発想は極めて単純で、**同じゲノムを読んでいるんだから、実際に出てくる配列のレパートリーはそんなに大きくないはず**だという仮定を活かす。アナロジーを用いて説明すると、ゲノムシークエンシングとは、要するに**大量の同じ本をシュレッダーに掛けて、ゴミ箱をがさっとすくってくること**なのだから、本質的には、それぞれの断片が、本のどの部分由来かを指してやれば、完全に表現できる。

さて、口で言うのは簡単だが、実際にやるのはそうでもない。読んだ生物種のゲノムが与えられていない状況では、状況はいっそう難しくなる。我々はゲノムを再構成してやるしかないのだろうか？　一方で、ゲノムの再構成(**本を復元する**)は、本当に時間が掛かり、本当に難しいことが経験的に分かっている。そうだ。圧縮はすぐ出来ないといけない。3ギガバイトのデータを1ギガバイトに圧縮するのに100時間も掛かっていたら、そのうち太陽系が溶けてしまう。ベデルギウスかなんかからガンマ線バーストがどうの。

肩の力を抜こう。どうせ完全な圧縮は出来ないのだ。

発想こうだ。各レコードから、指紋（比喩だ）を採取する。その後、同じ指紋を持っているレコードを集める。そのレコードを辞書順に並べる。後は、できるだけ重なりを大きくしながら取っていけばいい。

指紋の採り方は、minimizer という手法を使う。簡単に言えば、これは、レコードから長さ\\(k\\)の部分文字列をすべて生成して、その中にある辞書順で最小の要素だ。例えば、`AACCACGT`の3-minimizerと言ったら、`AAC`,`ACC`,`CCA`,`CAC`,`ACG`,`CGT`を辞書順にならべた先頭で、`AAC`になる。

（プロ向け：実は、minimizerは本来、配列を長さ\\(w)\\ずつのチャンクに分けて、その中で\\(k)\\長の部分文字列の最小要素を集めてくる。いわゆる\\(w,k\\）-minimizerというやつだ。このとき、辞書順に並べるのはあまりおすすめしない。もう少し賢い順序を経験的に定めた方が本当に良い。と言うのも、この世には`AAAA`みたいな配列がめちゃくちゃ多いので、指紋がダブる可能性がものすごく高いからだ）

さて、レコード集合を、minimizerを元にして分割できた。例えば、ある集合の要素は、みんな`AATCTAG`を持っている、という具合だ。

このあと、それぞれの集合のなかでソートを書ける。基数ソートを使おう。最後に、並んだリードを**もし、『ほとんど同じ、でも一文字だけ違う』と書く方が短いなら、そうしながら**、できるだけ重ねていけばエンコードは終わりだ。ふー。

（プロ向け：実際は、リードの後ろ十文字くらいはminimizerの計算からは外した方がいい。と言うのも、先頭にminimizerがあったばあい、かなりの圧縮が期待できるが、そうでない場合、結果はあまり芳しくならないためだ。また、リードの最後は読み取りを間違えやすいためでもある）


さて、面白いのが、彼らが圧縮後に行っている解析だ。彼らは、ヒトゲノムの全ゲノムシークエンシングを圧縮した結果、だいたい0.3 bit per base の圧縮率を達成したと言っている（塩基あたり0.3ビットまで圧縮できた）。

Q.何が面白いのか？ 

A.塩基は何種類あるだろうか？


質問に質問で返すな。

さて、塩基は四種類ある。*従って*、もし、塩基が偏り無くランダムに出るなら、一塩基あたりにかかるデータ量は2bitになる。エントロピーが2bitと言ってもいい。

一方で、圧縮の結果は、一塩基あたり0.3bitしか無いといっている。これはどういうことなのだろうか？　**圧縮率はエントロピーの上界を与える**のだ。

これは、非常に厚く読んでいっても、最終的には得られるエントロピーがどんどん小さくなっていくことの、実験的な証拠を与えていると私は考える。実際、論文の図によると、120x -- ゲノムを120回読んだのと同じだけのデータ -- においては、たったの0.2bit per base 程度の情報量しか残っていない。そして、これの大部分は、リードが内在的に持つ値、つまり**ゲノムのどの位置から始まったかを示すのにかかる情報量を、リードの長さで割ったもの**に近づいていく。

私は何を教訓にしようとしているのか？

1. 塩基配列そのものではなく、塩基配列がどう書かれているかを扱うことで、圧縮が達成できる。敷衍して言うと、解くべき問題は、実は我々が解かなければならないと思っている物と同じはない。
2. シークエンシングによって手に入る情報量は、収穫逓減の法則に従うように見える。我々は野放図にゲノムを読み過ぎている。

